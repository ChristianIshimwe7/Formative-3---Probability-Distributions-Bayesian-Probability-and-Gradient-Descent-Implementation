# Group Project: Probability Distributions, Bayesian Inference & Gradient Descent

This project enhances our comprehension of fundamental machine learning concepts by combining theoretical understanding with practical implementation. Our group‚Äôs primary focus is the **Poisson Distribution**, supplemented by analyses of Bayesian probability and gradient descent optimization.

---

##  Project Objectives

- Develop implementations of key probability distributions from first principles and analyze their distinguishing features.  
- Apply Bayesian probability to solve a real-world problem, demonstrating stepwise updates of inference.  
- Perform manual calculations of gradient descent updates in a simple linear regression context.  
- Translate manual gradient descent computations into Python code using SciPy, ensuring transparency in each iteration.  
- Employ Matplotlib to visualize results for enhanced interpretability.

---

##  Part 1: Probability Distributions ‚Äî Poisson Distribution (Our Group‚Äôs Focus)

Our objectives include:

- Analyzing the Poisson distribution as an effective model for rare event counts within fixed intervals.  
- Manually implementing the Poisson PMF and CDF in Python without external libraries.  
- Applying the distribution to a real-world problem, accompanied by a relevant question to illustrate its use.  
- Visualizing the distribution using Matplotlib for intuitive understanding.


---

##  Part 2: Bayesian Probability

We will:

- Identify a practical problem suitable for Bayesian inference, such as medical diagnosis or spam filtering.  
- Implement Bayes‚Äô theorem in Python, detailing the stepwise updates of prior, likelihood, and posterior probabilities.  
- Clarify the practical significance of Bayesian inference in real-world decision-making processes.

---

## Part 3: Gradient Descent ‚Äî Manual Calculation

Our work includes:

- Manually computing three iterative updates of gradient descent parameters (`m` and `b`) for a simple linear regression model.  
- Utilizing specified data points, initial parameter values, and a fixed learning rate.  
- Calculating predicted values, gradients using Mean Squared Error (MSE), and updated parameters at each iteration.  
- Documenting all intermediate steps clearly and analyzing the trend in error reduction.

*Each member of the group will complete at least one iteration.*

---

##  Part 4: Gradient Descent ‚Äî Python Implementation

We will:

- Convert the manual gradient descent calculations into Python code leveraging the SciPy library.  
- Explicitly update parameters `m` and `b` during each iteration, maintaining visibility into the optimization process.  
- Compute final predictions and visualize the changes in parameters and error over iterations.  
- Utilize Matplotlib to produce clear, separate plots depicting parameter convergence and error reduction.

---

## üõ† Tools and Technologies

- **Python 3** (primary programming language)  
- **SciPy** (numerical optimization for gradient descent)  
- **Matplotlib** (data visualization)  
- Manual, library-free implementations for probability distributions and Bayesian calculations

---

## Conclusion

This project provides a robust foundation in essential machine learning principles by linking theory to practice. Focusing on the Poisson distribution and advancing through Bayesian inference and gradient descent, we acquire critical skills for mastering machine learning fundamentals.

---

*‚ÄúA deep understanding of foundational mathematics is the key to becoming proficient machine learning practitioners.‚Äù*
